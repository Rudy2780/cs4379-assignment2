{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b6d4c52-af28-4f2b-98cc-45469c249d6b",
   "metadata": {},
   "source": [
    "# CS4379 Assignment 2: NumPy, Pandas, and Data I/O\n",
    "\n",
    "**Name:** Rudy Rutiaga  \n",
    "**Date:** February 2025\n",
    "\n",
    "This notebook covers NumPy fundamentals, pandas data manipulation, and efficient data I/O techniques.\n",
    "\n",
    "---\n",
    "\n",
    "## Part A: NumPy Fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d16adc5-87ee-495f-9b16-68ad46e49ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f6844a-c6a9-4be5-9199-a9c2adef91e0",
   "metadata": {},
   "source": [
    "### Problem 1: Array Creation and dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96b4027f-3905-40c4-b1cb-9a1678d4ccc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array a - Shape: (20,) dtype: int64\n",
      "Array b - Shape: (4, 5) dtype: int64\n",
      "Array c - Shape: (4, 5) dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create 1D array with integers 0-19\n",
    "a = np.arange(20)\n",
    "\n",
    "# Reshape into 2D array (4x5)\n",
    "b = a.reshape(4, 5)\n",
    "\n",
    "# Divide b by 3 to create float array\n",
    "c = b / 3\n",
    "\n",
    "# Print shapes and dtypes\n",
    "print(\"Array a - Shape:\", a.shape, \"dtype:\", a.dtype)\n",
    "print(\"Array b - Shape:\", b.shape, \"dtype:\", b.dtype)\n",
    "print(\"Array c - Shape:\", c.shape, \"dtype:\", c.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a13d894-04b7-4bb9-9694-d02e48dd1125",
   "metadata": {},
   "source": [
    "**Explanation:** Array `c` has dtype `float64` because when we divide an integer array by a number, NumPy automatically converts the result to floating-point to preserve decimal precision. Integer division would lose information (e.g., 1/3 would become 0), so NumPy uses float64 as the default type for division operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed06e9d-f7b1-490f-889b-46ecd1ec8ed5",
   "metadata": {},
   "source": [
    "### Problem 2: Indexing, Slicing, and Boolean Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8891c967-56b1-47e0-8eb2-8db5ff7e42e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last column: [ 4  9 -1 -1]\n",
      "Sub-matrix:\n",
      " [[ 7  8  9]\n",
      " [12 -1 -1]]\n",
      "Values divisible by 3: [ 0  3  6  9 12]\n",
      "Array b after replacement:\n",
      " [[ 0  1  2  3  4]\n",
      " [ 5  6  7  8  9]\n",
      " [10 11 12 -1 -1]\n",
      " [-1 -1 -1 -1 -1]]\n"
     ]
    }
   ],
   "source": [
    "# 1. Extract last column\n",
    "last_col = b[:, -1]  \n",
    "print(\"Last column:\", last_col)\n",
    "\n",
    "# 2. Extract sub-matrix (rows 1-2, columns 2-4)\n",
    "sub = b[1:3, 2:5]  \n",
    "print(\"Sub-matrix:\\n\", sub)\n",
    "\n",
    "# 3. Boolean mask for divisible by 3\n",
    "mask = b % 3 == 0 \n",
    "divisible_by_3 = b[mask]\n",
    "print(\"Values divisible by 3:\", divisible_by_3)\n",
    "\n",
    "# 4. Replace values > 12 with -1\n",
    "b[b > 12] = -1  \n",
    "print(\"Array b after replacement:\\n\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455a8d73-ca77-43d3-9b96-c2b630d08a85",
   "metadata": {},
   "source": [
    "**Explanation:** \n",
    "- Used negative indexing (`-1`) to extract the last column\n",
    "- Slicing `[1:3, 2:5]` extracts rows at index 1 and 2, columns at index 2, 3, and 4 (end is exclusive)\n",
    "- Boolean mask `b % 3 == 0` creates a True/False array, then `b[mask]` extracts only True positions\n",
    "- Boolean indexing `b[b > 12] = -1` finds all values greater than 12 and replaces them in place without loops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd118096-87c9-4fa0-b41d-7cab8545d8cc",
   "metadata": {},
   "source": [
    "### Problem 3: Vectorized Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78ab8e9e-1c6a-456a-82b6-210298854391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of z: 2.886579864025407e-17\n",
      "Std of z: 0.99498743710662\n",
      "Proportion in [-1, 1]: 0.7\n",
      "\n",
      "Random seed used: 27\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(27)  \n",
    "\n",
    "# Create array of 100 random values from standard normal\n",
    "x = np.random.normal(size=100)  \n",
    "\n",
    "# Compute z-scores\n",
    "x_mean = np.mean(x)  # calculate mean of x\n",
    "x_std = np.std(x, ddof=1)   # calculate std of x\n",
    "z = (x - x_mean) / x_std\n",
    "\n",
    "# Verify z-scores\n",
    "print(f\"Mean of z: {z.mean()}\")\n",
    "print(f\"Std of z: {z.std()}\")\n",
    "\n",
    "# Proportion in [-1, 1]\n",
    "in_range = (z >= -1) & (z <= 1)  \n",
    "proportion = in_range.sum()/len(z)  \n",
    "print(f\"Proportion in [-1, 1]: {proportion}\")\n",
    "\n",
    "print(f\"\\nRandom seed used: 27\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5f743f-09c8-49e7-af0d-d4bf9f80c41d",
   "metadata": {},
   "source": [
    "**Explanation:** Z-scores standardize data by subtracting the mean and dividing by standard deviation. The resulting z-scores have mean ≈ 0 and std ≈ 1 by definition. The small mean value (2.89e-17) is effectively zero due to floating-point precision. About 70% of values fall within [-1, 1], which aligns with the empirical rule that approximately 68% of a normal distribution lies within one standard deviation of the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7073de24-d254-4169-ac86-a3ea6994d286",
   "metadata": {},
   "source": [
    "### Problem 4: Aggregation and Axis Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eed71415-4ac8-42ec-9652-09311716a704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix M:\n",
      "[[5 2 2 2]\n",
      " [2 2 3 3]\n",
      " [4 4 3 4]\n",
      " [3 6 5 5]\n",
      " [4 2 5 6]\n",
      " [5 4 6 4]]\n",
      "\n",
      "Row sums: [11 10 15 19 17 19]\n",
      "Column means: [3.83333333 3.33333333 4.         4.        ]\n",
      "Max value index (row, col): (np.int64(3), np.int64(1))\n",
      "\n",
      "Standardized M:\n",
      "[[ 0.99796541 -0.81649658 -1.29099445 -1.41421356]\n",
      " [-1.56823136 -0.81649658 -0.64549722 -0.70710678]\n",
      " [ 0.14256649  0.40824829 -0.64549722  0.        ]\n",
      " [-0.71283244  1.63299316  0.64549722  0.70710678]\n",
      " [ 0.14256649 -0.81649658  0.64549722  1.41421356]\n",
      " [ 0.99796541  0.40824829  1.29099445  0.        ]]\n",
      "\n",
      "Verify - Column means: [-1.29526020e-16 -9.25185854e-17 -3.70074342e-17  0.00000000e+00]\n",
      "Verify - Column stds: [1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Set seed\n",
    "np.random.seed(27)  \n",
    "\n",
    "# Create 6x4 array with random integers 0-9\n",
    "M = np.random.randint(2, 7, size=(6, 4))\n",
    "print(\"Matrix M:\")\n",
    "print(M)\n",
    "print()\n",
    "\n",
    "# 1. Sum of each row\n",
    "row_sums = M.sum(axis=1)  \n",
    "print(\"Row sums:\", row_sums)\n",
    "\n",
    "# 2. Mean of each column  \n",
    "col_means = M.mean(axis=0)  \n",
    "print(\"Column means:\", col_means)\n",
    "\n",
    "# 3. Index of maximum value\n",
    "max_idx = np.unravel_index(M.argmax(), M.shape)\n",
    "print(f\"Max value index (row, col): {max_idx}\")\n",
    "\n",
    "# 4. Column-wise standardization\n",
    "M_standardized = (M - M.mean(axis=0)) / M.std(axis=0, ddof=1)\n",
    "print(\"\\nStandardized M:\")\n",
    "print(M_standardized)\n",
    "print(\"\\nVerify - Column means:\", M_standardized.mean(axis=0))\n",
    "print(\"Verify - Column stds:\", M_standardized.std(axis=0, ddof=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ab9599-4657-4a59-b071-f4b68401e86b",
   "metadata": {},
   "source": [
    "**Explanation:** The `axis` parameter determines the direction of aggregation. `axis=1` collapses across columns (giving row-wise results), while `axis=0` collapses down rows (giving column-wise results). For standardization, we subtracted column means and divided by column standard deviations using broadcasting, which automatically applies the operation element-wise. The verification shows all columns now have mean ≈ 0 (small values due to floating-point precision) and std = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c332e0b3-6d59-4288-a6fd-c85ed53857a0",
   "metadata": {},
   "source": [
    "### Problem 5: Linear Algebra Basics (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba0aa94c-1bfd-475f-8bb5-e37d728e5a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A*v = [ 4 10  8]\n",
      "Determinant of A: 8.000000000000002\n",
      "Eigenvalues of A: [4. 2. 1.]\n",
      "Solution x: [0.5 0.  1.5]\n",
      "Verification A*x = [1. 2. 3.]\n"
     ]
    }
   ],
   "source": [
    "# Define matrix A and vector v\n",
    "A = np.array([[2, 1, 0],\n",
    "              [1, 3, 1],\n",
    "              [0, 1, 2]])\n",
    "\n",
    "v = np.array([1, 2, 3])\n",
    "\n",
    "# 1. Compute A*v (matrix-vector product)\n",
    "Av = np.dot(A, v)  \n",
    "print(\"A*v =\", Av)\n",
    "\n",
    "# 2. Determinant of A\n",
    "det_A = np.linalg.det(A)  \n",
    "print(f\"Determinant of A: {det_A}\")\n",
    "\n",
    "# 3. Eigenvalues of A\n",
    "eigenvalues = np.linalg.eigvals(A)  \n",
    "print(f\"Eigenvalues of A: {eigenvalues}\")\n",
    "\n",
    "# 4. Solve Ax = v for x\n",
    "x = np.linalg.solve(A, v)  \n",
    "print(f\"Solution x: {x}\")\n",
    "\n",
    "# Verify the solution\n",
    "print(f\"Verification A*x = {np.dot(A, x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48b42c4-1742-4cca-818d-d8f1268e5971",
   "metadata": {},
   "source": [
    "**Explanation:** \n",
    "- Matrix-vector multiplication A·v gives [4, 10, 8]\n",
    "- Determinant of A is 8 (slightly off due to floating-point precision)\n",
    "- Eigenvalues are [4, 2, 1]\n",
    "- Solution to Ax = v is x = [0.5, 0, 1.5], verified by computing A·x = v\n",
    "\n",
    "**When to use `np.linalg.solve()` vs computing A⁻¹:** \n",
    "Use `np.linalg.solve(A, v)` instead of computing `inv(A) @ v` because it's more numerically stable and computationally efficient. Direct inversion can amplify rounding errors and is slower, especially for large matrices. `solve()` uses optimized algorithms (like LU decomposition) that are both faster and more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fb9716-fdc1-4b44-96a6-0cd055cdde85",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part B: Pandas\n",
    "\n",
    "### Dataset Creation\n",
    "\n",
    "Creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d88b81e3-e6e0-47a6-996a-1e1c1bb9403a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created successfully\n",
      "   order_id        date customer     city category  units  unit_price\n",
      "0       101  2026-01-03      Ava   Boston    Books      2       12.50\n",
      "1       102  2026-01-03     Noah   Boston    Games      1       59.99\n",
      "2       103  2026-01-04      Ava  Seattle    Books      3       12.50\n",
      "3       104  2026-01-05      Mia  Seattle   Garden      4        7.25\n",
      "4       105  2026-01-06     Liam   Austin    Books      1       20.00\n",
      "5       106  2026-01-06     Noah   Austin   Garden      2        7.25\n",
      "6       107  2026-01-07      Mia   Boston    Games      2       59.99\n",
      "7       108  2026-01-08      Ava   Austin    Books      2       12.50\n"
     ]
    }
   ],
   "source": [
    "records = [\n",
    "    {\"order_id\": 101, \"date\": \"2026-01-03\", \"customer\": \"Ava\", \"city\": \"Boston\", \"category\": \"Books\", \"units\": 2, \"unit_price\": 12.50},\n",
    "    {\"order_id\": 102, \"date\": \"2026-01-03\", \"customer\": \"Noah\", \"city\": \"Boston\", \"category\": \"Games\", \"units\": 1, \"unit_price\": 59.99},\n",
    "    {\"order_id\": 103, \"date\": \"2026-01-04\", \"customer\": \"Ava\", \"city\": \"Seattle\", \"category\": \"Books\", \"units\": 3, \"unit_price\": 12.50},\n",
    "    {\"order_id\": 104, \"date\": \"2026-01-05\", \"customer\": \"Mia\", \"city\": \"Seattle\", \"category\": \"Garden\", \"units\": 4, \"unit_price\": 7.25},\n",
    "    {\"order_id\": 105, \"date\": \"2026-01-06\", \"customer\": \"Liam\", \"city\": \"Austin\", \"category\": \"Books\", \"units\": 1, \"unit_price\": 20.00},\n",
    "    {\"order_id\": 106, \"date\": \"2026-01-06\", \"customer\": \"Noah\", \"city\": \"Austin\", \"category\": \"Garden\", \"units\": 2, \"unit_price\": 7.25},\n",
    "    {\"order_id\": 107, \"date\": \"2026-01-07\", \"customer\": \"Mia\", \"city\": \"Boston\", \"category\": \"Games\", \"units\": 2, \"unit_price\": 59.99},\n",
    "    {\"order_id\": 108, \"date\": \"2026-01-08\", \"customer\": \"Ava\", \"city\": \"Austin\", \"category\": \"Books\", \"units\": 2, \"unit_price\": 12.50},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "print(\"Dataset created successfully\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5daa34-c903-4e49-965d-03c4e560c9a0",
   "metadata": {},
   "source": [
    "### Problem 6: Basic DataFrame Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06986dbb-8686-486b-9e43-941d933560e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows:\n",
      "   order_id        date customer     city category  units  unit_price\n",
      "0       101  2026-01-03      Ava   Boston    Books      2       12.50\n",
      "1       102  2026-01-03     Noah   Boston    Games      1       59.99\n",
      "2       103  2026-01-04      Ava  Seattle    Books      3       12.50\n",
      "3       104  2026-01-05      Mia  Seattle   Garden      4        7.25\n",
      "4       105  2026-01-06     Liam   Austin    Books      1       20.00\n",
      "\n",
      "DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8 entries, 0 to 7\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   order_id    8 non-null      int64  \n",
      " 1   date        8 non-null      object \n",
      " 2   customer    8 non-null      object \n",
      " 3   city        8 non-null      object \n",
      " 4   category    8 non-null      object \n",
      " 5   units       8 non-null      int64  \n",
      " 6   unit_price  8 non-null      float64\n",
      "dtypes: float64(1), int64(2), object(4)\n",
      "memory usage: 580.0+ bytes\n",
      "\n",
      "After date conversion:\n",
      "order_id               int64\n",
      "date          datetime64[ns]\n",
      "customer              object\n",
      "city                  object\n",
      "category              object\n",
      "units                  int64\n",
      "unit_price           float64\n",
      "dtype: object\n",
      "\n",
      "DataFrame with revenue:\n",
      "   order_id       date customer     city category  units  unit_price  revenue\n",
      "0       101 2026-01-03      Ava   Boston    Books      2       12.50    25.00\n",
      "1       102 2026-01-03     Noah   Boston    Games      1       59.99    59.99\n",
      "2       103 2026-01-04      Ava  Seattle    Books      3       12.50    37.50\n",
      "3       104 2026-01-05      Mia  Seattle   Garden      4        7.25    29.00\n",
      "4       105 2026-01-06     Liam   Austin    Books      1       20.00    20.00\n",
      "5       106 2026-01-06     Noah   Austin   Garden      2        7.25    14.50\n",
      "6       107 2026-01-07      Mia   Boston    Games      2       59.99   119.98\n",
      "7       108 2026-01-08      Ava   Austin    Books      2       12.50    25.00\n"
     ]
    }
   ],
   "source": [
    "# 1. Show head and info\n",
    "print(\"First 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nDataFrame info:\")\n",
    "df.info()\n",
    "\n",
    "# 2. Convert date to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "print(\"\\nAfter date conversion:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# 3. Add revenue column\n",
    "df['revenue'] = df['units'] * df['unit_price']\n",
    "print(\"\\nDataFrame with revenue:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13038868-08ce-4a27-a655-45d736452bd1",
   "metadata": {},
   "source": [
    "**Explanation:** \n",
    "- `df.head()` displays the first 5 rows to preview the data\n",
    "- `df.info()` shows column names, data types, and memory usage\n",
    "- Converting `date` from object (string) to datetime64 enables date-specific operations like filtering by date ranges or extracting month/year\n",
    "- The `revenue` column is calculated by element-wise multiplication of `units` and `unit_price` columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a9ac3d-3e29-4df8-9438-374eefeb86b8",
   "metadata": {},
   "source": [
    "### Problem 7: Filtering and Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "77a318f8-d1c8-452d-996e-ff99bce3404a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   order_id       date     city category  revenue\n",
      "1       102 2026-01-03   Boston    Games    59.99\n",
      "0       101 2026-01-03   Boston    Books    25.00\n",
      "3       104 2026-01-05  Seattle   Garden    29.00\n",
      "5       106 2026-01-06   Austin   Garden    14.50\n",
      "6       107 2026-01-07   Boston    Games   119.98\n"
     ]
    }
   ],
   "source": [
    "# Use the boolean mask to filter df\n",
    "filtered = df[(df['city'] == 'Boston') | (df['category'] == 'Garden')]\n",
    "\n",
    "# Now sort the filtered DataFrame\n",
    "sorted_df = filtered.sort_values(by=['date', 'revenue'], ascending=[True, False])\n",
    "\n",
    "# Select columns\n",
    "result = sorted_df[['order_id', 'date', 'city', 'category', 'revenue']]\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be62621-4260-4041-94a6-313e693836a5",
   "metadata": {},
   "source": [
    "**Explanation:** \n",
    "- Used the `|` (OR) operator to filter rows where city is Boston OR category is Garden\n",
    "- The boolean condition creates a True/False mask that selects matching rows\n",
    "- `sort_values()` with a list sorts by multiple columns: first by date ascending, then by revenue descending within each date\n",
    "- Selected only the required columns using double bracket notation `df[['col1', 'col2']]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600ffb13-ca8b-46bb-8dab-8f0b7ae6b97b",
   "metadata": {},
   "source": [
    "### Problem 8: GroupBy Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c878c4d-92f3-4020-94b2-849d5d7ae8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         total_revenue  total_units  distinct_customers  avg_unit_price\n",
      "city                                                                   \n",
      "Boston          204.97            5                   3          44.160\n",
      "Seattle          66.50            7                   2           9.875\n",
      "Austin           59.50            5                   3          13.250\n"
     ]
    }
   ],
   "source": [
    "# Group by city and aggregate multiple columns\n",
    "summary = df.groupby('city').agg({\n",
    "    'revenue': 'sum',  \n",
    "    'units': 'sum',        \n",
    "    'customer': 'nunique',    \n",
    "    'unit_price': 'mean'    \n",
    "})\n",
    "\n",
    "# Rename columns for clarity\n",
    "summary.columns = ['total_revenue', 'total_units', 'distinct_customers', 'avg_unit_price']\n",
    "\n",
    "# Sort by total_revenue descending\n",
    "summary = summary.sort_values('total_revenue', ascending=False)\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f2549b-0b73-4acc-af89-f6f18bdd0724",
   "metadata": {},
   "source": [
    "**Explanation:** \n",
    "- `groupby('city')` groups all orders by city\n",
    "- `.agg()` allows applying different aggregation functions to different columns in one call\n",
    "- `'nunique'` counts the number of unique customers per city (2 in Seattle, 3 in Boston and Austin)\n",
    "- Boston has the highest total revenue ($204.97) due to high-value Games purchases, despite having fewer total units than Seattle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d71d1e7-fe91-4ac4-9286-24a332a08e85",
   "metadata": {},
   "source": [
    "### Problem 9: Pivot Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca53bb3a-de1a-4087-ad90-d16bb2a6ecbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category    Books   Games  Garden\n",
      "date                             \n",
      "2026-01-03   25.0   59.99     0.0\n",
      "2026-01-04   37.5    0.00     0.0\n",
      "2026-01-05    0.0    0.00    29.0\n",
      "2026-01-06   20.0    0.00    14.5\n",
      "2026-01-07    0.0  119.98     0.0\n",
      "2026-01-08   25.0    0.00     0.0\n"
     ]
    }
   ],
   "source": [
    "# Create pivot table\n",
    "pivot = df.pivot_table(\n",
    "    index='date',    \n",
    "    columns='category',     \n",
    "    values='revenue',      \n",
    "    aggfunc='sum',      \n",
    "    fill_value=0       \n",
    ")\n",
    "\n",
    "print(pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b8ade0-8016-4e29-bd73-741f50319062",
   "metadata": {},
   "source": [
    "**Explanation:** \n",
    "- `pivot_table()` reshapes data with dates as rows and categories as columns\n",
    "- Each cell shows the total revenue for that date-category combination\n",
    "- `fill_value=0` replaces NaN with 0 for dates when a category had no sales\n",
    "- This format makes it easy to see sales patterns across categories over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82d2fe8-07f7-4289-98f3-93f9897e83ae",
   "metadata": {},
   "source": [
    "### Problem 10: Merging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8f945d55-5019-492f-9a0a-705c34bbd06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer tiers:\n",
      "  customer    tier\n",
      "0      Ava    Gold\n",
      "1     Noah  Silver\n",
      "2      Mia    Gold\n",
      "3     Liam  Bronze\n",
      "\n",
      "Merged DataFrame:\n",
      "   order_id       date customer     city category  units  unit_price  revenue  \\\n",
      "0       101 2026-01-03      Ava   Boston    Books      2       12.50    25.00   \n",
      "1       102 2026-01-03     Noah   Boston    Games      1       59.99    59.99   \n",
      "2       103 2026-01-04      Ava  Seattle    Books      3       12.50    37.50   \n",
      "3       104 2026-01-05      Mia  Seattle   Garden      4        7.25    29.00   \n",
      "4       105 2026-01-06     Liam   Austin    Books      1       20.00    20.00   \n",
      "5       106 2026-01-06     Noah   Austin   Garden      2        7.25    14.50   \n",
      "6       107 2026-01-07      Mia   Boston    Games      2       59.99   119.98   \n",
      "7       108 2026-01-08      Ava   Austin    Books      2       12.50    25.00   \n",
      "\n",
      "     tier  \n",
      "0    Gold  \n",
      "1  Silver  \n",
      "2    Gold  \n",
      "3    Gold  \n",
      "4  Bronze  \n",
      "5  Silver  \n",
      "6    Gold  \n",
      "7    Gold  \n",
      "\n",
      "Total revenue by tier:\n",
      "tier\n",
      "Gold      236.48\n",
      "Silver     74.49\n",
      "Bronze     20.00\n",
      "Name: revenue, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create customer tier DataFrame\n",
    "customer_tier = pd.DataFrame({\n",
    "    \"customer\": [\"Ava\", \"Noah\", \"Mia\", \"Liam\"],\n",
    "    \"tier\": [\"Gold\", \"Silver\", \"Gold\", \"Bronze\"]\n",
    "})\n",
    "\n",
    "print(\"Customer tiers:\")\n",
    "print(customer_tier)\n",
    "print()\n",
    "\n",
    "# Merge with orders DataFrame\n",
    "df_merged = df.merge(customer_tier, on='customer', how='left')\n",
    "print(\"Merged DataFrame:\")\n",
    "print(df_merged)\n",
    "print()\n",
    "\n",
    "# Compute total revenue by tier\n",
    "revenue_by_tier = df_merged.groupby('tier')['revenue'].sum().sort_values(ascending=False)\n",
    "print(\"Total revenue by tier:\")\n",
    "print(revenue_by_tier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b697a2-47f5-4de0-9996-14464a86f760",
   "metadata": {},
   "source": [
    "**Explanation:** Merged the orders DataFrame with customer_tier using a left join on the 'customer' column, adding tier information to each order. Gold tier generates the most revenue, followed by Silver and Bronze."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e29cdf-bc41-47e6-aa39-de72be58d0aa",
   "metadata": {},
   "source": [
    "### Problem 11: Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3b75f0a0-b341-4265-a548-9e9c057374c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After setting unit_price to NaN for order 106:\n",
      "   order_id       date customer     city category  units  unit_price  revenue\n",
      "0       101 2026-01-03      Ava   Boston    Books      2       12.50    25.00\n",
      "1       102 2026-01-03     Noah   Boston    Games      1       59.99    59.99\n",
      "2       103 2026-01-04      Ava  Seattle    Books      3       12.50    37.50\n",
      "3       104 2026-01-05      Mia  Seattle   Garden      4        7.25    29.00\n",
      "4       105 2026-01-06     Liam   Austin    Books      1       20.00    20.00\n",
      "5       106 2026-01-06     Noah   Austin   Garden      2         NaN    14.50\n",
      "6       107 2026-01-07      Mia   Boston    Games      2       59.99   119.98\n",
      "7       108 2026-01-08      Ava   Austin    Books      2       12.50    25.00\n",
      "\n",
      "Missing values per column:\n",
      "order_id      0\n",
      "date          0\n",
      "customer      0\n",
      "city          0\n",
      "category      0\n",
      "units         0\n",
      "unit_price    1\n",
      "revenue       0\n",
      "dtype: int64\n",
      "\n",
      "After filling missing unit_price with category median:\n",
      "   order_id category  unit_price\n",
      "0       101    Books       12.50\n",
      "1       102    Games       59.99\n",
      "2       103    Books       12.50\n",
      "3       104   Garden        7.25\n",
      "4       105    Books       20.00\n",
      "5       106   Garden        7.25\n",
      "6       107    Games       59.99\n",
      "7       108    Books       12.50\n",
      "\n",
      "Order 106 revenue after fix:\n",
      "   order_id  units  unit_price  revenue\n",
      "5       106      2        7.25     14.5\n"
     ]
    }
   ],
   "source": [
    "# Make a copy of df\n",
    "df2 = df.copy()\n",
    "\n",
    "# 1. Set unit_price to NaN for order_id 106\n",
    "df2.loc[df2['order_id'] == 106, 'unit_price'] = np.nan\n",
    "print(\"After setting unit_price to NaN for order 106:\")\n",
    "print(df2)\n",
    "print()\n",
    "\n",
    "# 2. Count missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df2.isnull().sum())\n",
    "print()\n",
    "\n",
    "# 3. Replace missing unit_price with median within same category\n",
    "df2['unit_price'] = df2.groupby('category')['unit_price'].transform(lambda x: x.fillna(x.median()))\n",
    "print(\"After filling missing unit_price with category median:\")\n",
    "print(df2[['order_id', 'category', 'unit_price']])\n",
    "print()\n",
    "\n",
    "# 4. Recompute revenue\n",
    "df2['revenue'] = df2['units'] * df2['unit_price']\n",
    "print(\"Order 106 revenue after fix:\")\n",
    "print(df2[df2['order_id'] == 106][['order_id', 'units', 'unit_price', 'revenue']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0f5d17-40b2-4884-8c2e-2cc4d24237bd",
   "metadata": {},
   "source": [
    "**Explanation:** Created a copy to avoid modifying the original. Set order 106's unit_price to NaN, then filled it with the median unit_price of the same category (Garden). The `groupby().transform()` pattern applies the filling operation within each group while maintaining the original DataFrame shape. Revenue was successfully recomputed for order 106."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9499e2-ba2f-4795-afdc-780857abe029",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part C: Data I/O and Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e3385d-0d51-4b69-b763-41423cf922f7",
   "metadata": {},
   "source": [
    "### Problem 12: CSV I/O with Schema Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5281ef5d-0bcc-4e2f-9198-932e3c5a09f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written to orders.csv\n",
      "\n",
      "DataFrame dtypes after reading:\n",
      "order_id               int64\n",
      "date          datetime64[ns]\n",
      "customer              object\n",
      "city                  object\n",
      "category              object\n",
      "units                  int64\n",
      "unit_price           float64\n",
      "revenue              float64\n",
      "dtype: object\n",
      "\n",
      "First few rows:\n",
      "   order_id       date customer     city category  units  unit_price  revenue\n",
      "0       101 2026-01-03      Ava   Boston    Books      2       12.50    25.00\n",
      "1       102 2026-01-03     Noah   Boston    Games      1       59.99    59.99\n",
      "2       103 2026-01-04      Ava  Seattle    Books      3       12.50    37.50\n",
      "3       104 2026-01-05      Mia  Seattle   Garden      4        7.25    29.00\n",
      "4       105 2026-01-06     Liam   Austin    Books      1       20.00    20.00\n"
     ]
    }
   ],
   "source": [
    "# 1. Write df to CSV without index\n",
    "df.to_csv('../data/orders.csv', index=False)\n",
    "print(\"Written to orders.csv\")\n",
    "\n",
    "# 2. Read it back with proper dtypes\n",
    "df_csv = pd.read_csv('../data/orders.csv', \n",
    "                      parse_dates=['date'],\n",
    "                      dtype={'order_id': 'int64'})\n",
    "\n",
    "# 3. Confirm dtypes\n",
    "print(\"\\nDataFrame dtypes after reading:\")\n",
    "print(df_csv.dtypes)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df_csv.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbea4a1c-cc0f-475e-b787-48496fc5ccd1",
   "metadata": {},
   "source": [
    "**Explanation:** Wrote the DataFrame to CSV without the index column to avoid creating an unnecessary numbered column. When reading back, explicitly parsed 'date' as datetime and ensured 'order_id' is int64 to maintain data types. This prevents common issues like dates being read as strings or IDs as floats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1d3ec6-0f39-42e2-843b-6808ac17e7ac",
   "metadata": {},
   "source": [
    "### Problem 13: Efficient CSV Reading for Larger Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "61e074b9-1912-45d5-a921-07a89e757c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sensor_data_large.csv with 200000 rows\n",
      "\n",
      "Memory usage with usecols: 11.63 MB\n",
      "Shape: (200000, 2)\n",
      "\n",
      "Average temperature per sensor:\n",
      "  sensor_id  avg_temperature\n",
      "0      S001        22.021851\n",
      "1      S002        21.990635\n",
      "2      S003        22.003920\n",
      "3      S004        21.965210\n",
      "4      S005        22.044571\n"
     ]
    }
   ],
   "source": [
    "# 1. Generate synthetic sensor data\n",
    "np.random.seed(42)\n",
    "n_rows = 200000\n",
    "\n",
    "sensor_data = pd.DataFrame({\n",
    "    'timestamp': pd.date_range('2026-01-01', periods=n_rows, freq='1min'),\n",
    "    'sensor_id': np.random.choice(['S001', 'S002', 'S003', 'S004', 'S005'], n_rows),\n",
    "    'temperature': np.random.normal(22, 5, n_rows),\n",
    "    'humidity': np.random.uniform(30, 70, n_rows),\n",
    "    'pressure': np.random.normal(1013, 10, n_rows),\n",
    "    'voltage': np.random.uniform(3.0, 3.3, n_rows),\n",
    "    'status': np.random.choice(['OK', 'WARNING', 'ERROR'], n_rows, p=[0.9, 0.08, 0.02]),\n",
    "    'location': np.random.choice(['Building A', 'Building B', 'Building C'], n_rows),\n",
    "    'reading_quality': np.random.uniform(0, 100, n_rows),\n",
    "    'battery_pct': np.random.uniform(0, 100, n_rows)\n",
    "})\n",
    "\n",
    "sensor_data.to_csv('../data/sensor_data_large.csv', index=False)\n",
    "print(f\"Generated sensor_data_large.csv with {n_rows} rows\")\n",
    "\n",
    "# 2. Read only sensor_id and temperature columns\n",
    "df_subset = pd.read_csv('../data/sensor_data_large.csv', usecols=['sensor_id', 'temperature'])\n",
    "memory_mb = df_subset.memory_usage(deep=True).sum() / (1024**2)\n",
    "print(f\"\\nMemory usage with usecols: {memory_mb:.2f} MB\")\n",
    "print(f\"Shape: {df_subset.shape}\")\n",
    "\n",
    "# 3. Compute average temperature per sensor using chunking\n",
    "chunk_size = 50000\n",
    "sensor_sums = {}\n",
    "sensor_counts = {}\n",
    "\n",
    "for chunk in pd.read_csv('../data/sensor_data_large.csv', \n",
    "                          usecols=['sensor_id', 'temperature'],\n",
    "                          chunksize=chunk_size):\n",
    "    grouped = chunk.groupby('sensor_id')['temperature'].agg(['sum', 'count'])\n",
    "    \n",
    "    for sensor in grouped.index:\n",
    "        sensor_sums[sensor] = sensor_sums.get(sensor, 0) + grouped.loc[sensor, 'sum']\n",
    "        sensor_counts[sensor] = sensor_counts.get(sensor, 0) + grouped.loc[sensor, 'count']\n",
    "\n",
    "avg_temp = pd.DataFrame({\n",
    "    'sensor_id': list(sensor_sums.keys()),\n",
    "    'avg_temperature': [sensor_sums[s] / sensor_counts[s] for s in sensor_sums.keys()]\n",
    "}).sort_values('sensor_id')\n",
    "\n",
    "print(\"\\nAverage temperature per sensor:\")\n",
    "print(avg_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c4d506-c3d1-499b-aa04-7f0ad73afab0",
   "metadata": {},
   "source": [
    "**Explanation:** \n",
    "Generated a 200,000-row synthetic dataset. Using `usecols` loaded only 2 columns instead of 10, significantly reducing memory usage. Chunked reading processes the file in 50,000-row batches, computing partial aggregations and combining them. This is helpful even for files that fit in memory because it:\n",
    "1. Reduces peak memory usage (important on memory-constrained systems)\n",
    "2. Allows processing files larger than available RAM\n",
    "3. Enables early termination if you find what you need in early chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f2f8b2-8e54-4acf-ace1-83e46118632c",
   "metadata": {},
   "source": [
    "### Problem 14: JSON Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "14d5eec3-bc14-4522-80eb-3069a9841850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported orders_records.json\n",
      "Exported orders_table.json\n",
      "\n",
      "From records format:\n",
      "   order_id       date customer     city category  units  unit_price  revenue\n",
      "0       101 2026-01-03      Ava   Boston    Books      2       12.50    25.00\n",
      "1       102 2026-01-03     Noah   Boston    Games      1       59.99    59.99\n",
      "2       103 2026-01-04      Ava  Seattle    Books      3       12.50    37.50\n",
      "3       104 2026-01-05      Mia  Seattle   Garden      4        7.25    29.00\n",
      "4       105 2026-01-06     Liam   Austin    Books      1       20.00    20.00\n",
      "\n",
      "From table format:\n",
      "   order_id       date customer     city category  units  unit_price  revenue\n",
      "0       101 2026-01-03      Ava   Boston    Books      2       12.50    25.00\n",
      "1       102 2026-01-03     Noah   Boston    Games      1       59.99    59.99\n",
      "2       103 2026-01-04      Ava  Seattle    Books      3       12.50    37.50\n",
      "3       104 2026-01-05      Mia  Seattle   Garden      4        7.25    29.00\n",
      "4       105 2026-01-06     Liam   Austin    Books      1       20.00    20.00\n",
      "\n",
      "Records matches original: True\n",
      "Table matches original: True\n"
     ]
    }
   ],
   "source": [
    "# 1. Export as records format\n",
    "df.to_json('../data/orders_records.json', orient='records', indent=2)\n",
    "print(\"Exported orders_records.json\")\n",
    "\n",
    "# 2. Export as table format\n",
    "df.to_json('../data/orders_table.json', orient='table', indent=2)\n",
    "print(\"Exported orders_table.json\")\n",
    "\n",
    "# 3. Read both back\n",
    "df_records = pd.read_json('../data/orders_records.json', orient='records')\n",
    "df_table = pd.read_json('../data/orders_table.json', orient='table')\n",
    "\n",
    "print(\"\\nFrom records format:\")\n",
    "print(df_records.head())\n",
    "print(\"\\nFrom table format:\")\n",
    "print(df_table.head())\n",
    "\n",
    "# Verify they match original\n",
    "print(\"\\nRecords matches original:\", df_records.equals(df[df_records.columns]))\n",
    "print(\"Table matches original:\", df_table.equals(df[df_table.columns]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9b6d81-f1fb-407a-9645-3d25894b2d43",
   "metadata": {},
   "source": [
    "**Explanation:** \n",
    "The 'records' format creates a simple JSON array of objects, making it easy to integrate with web APIs and JavaScript applications. The 'table' format includes schema metadata (column names, data types, index info), making it better for data archival and ensuring data types are preserved when reloading. Records is more portable, while table is more robust for round-trip serialization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45b0f08-2e8c-4275-be5d-8bf9860b8ba3",
   "metadata": {},
   "source": [
    "### Problem 15: Robust Path Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ceb297ad-1db7-44af-b29e-fed2e2231def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensured ../data exists\n",
      "Written to ../data/orders.csv\n",
      "\n",
      "Read back from ../data/orders.csv\n",
      "   order_id       date customer     city category  units  unit_price  revenue\n",
      "0       101 2026-01-03      Ava   Boston    Books      2       12.50    25.00\n",
      "1       102 2026-01-03     Noah   Boston    Games      1       59.99    59.99\n",
      "2       103 2026-01-04      Ava  Seattle    Books      3       12.50    37.50\n",
      "3       104 2026-01-05      Mia  Seattle   Garden      4        7.25    29.00\n",
      "4       105 2026-01-06     Liam   Austin    Books      1       20.00    20.00\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# 1. Create data folder if it doesn't exist\n",
    "data_dir = Path('../data')\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Ensured {data_dir} exists\")\n",
    "\n",
    "# 2. Write df to data/orders.csv\n",
    "output_path = data_dir / 'orders.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Written to {output_path}\")\n",
    "\n",
    "# 3. Read it back\n",
    "df_read = pd.read_csv(output_path, parse_dates=['date'])\n",
    "print(f\"\\nRead back from {output_path}\")\n",
    "print(df_read.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d740572-89a4-4367-b29d-10ea9557d5af",
   "metadata": {},
   "source": [
    "**Explanation:** Using `pathlib.Path` provides cross-platform path handling without hard-coding separators like `/` or `\\`. The `/` operator joins paths correctly on any OS. `mkdir(parents=True, exist_ok=True)` creates the directory if needed without raising errors if it already exists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d154a70-dc6a-432a-8d13-f9a349519919",
   "metadata": {},
   "source": [
    "### Problem 16: Auto-loader Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5bd794e7-819c-453b-b6c9-be99d298e3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from CSV:\n",
      "   order_id       date customer     city category  units  unit_price  revenue\n",
      "0       101 2026-01-03      Ava   Boston    Books      2       12.50    25.00\n",
      "1       102 2026-01-03     Noah   Boston    Games      1       59.99    59.99\n",
      "2       103 2026-01-04      Ava  Seattle    Books      3       12.50    37.50\n",
      "3       104 2026-01-05      Mia  Seattle   Garden      4        7.25    29.00\n",
      "4       105 2026-01-06     Liam   Austin    Books      1       20.00    20.00\n",
      "\n",
      "Loading from JSON (records):\n",
      "   order_id       date customer     city category  units  unit_price  revenue\n",
      "0       101 2026-01-03      Ava   Boston    Books      2       12.50    25.00\n",
      "1       102 2026-01-03     Noah   Boston    Games      1       59.99    59.99\n",
      "2       103 2026-01-04      Ava  Seattle    Books      3       12.50    37.50\n",
      "3       104 2026-01-05      Mia  Seattle   Garden      4        7.25    29.00\n",
      "4       105 2026-01-06     Liam   Austin    Books      1       20.00    20.00\n",
      "\n",
      "Function successfully loads both CSV and JSON formats!\n"
     ]
    }
   ],
   "source": [
    "def load_orders(path):\n",
    "    \"\"\"\n",
    "    Load orders data from CSV or JSON file.\n",
    "    \n",
    "    Parameters:\n",
    "    path (str or Path): Path to the data file\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Loaded orders data\n",
    "    \n",
    "    Raises:\n",
    "    ValueError: If file extension is not .csv or .json\n",
    "    \"\"\"\n",
    "    path = Path(path)\n",
    "    \n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "    \n",
    "    extension = path.suffix.lower()\n",
    "    \n",
    "    if extension == '.csv':\n",
    "        return pd.read_csv(path, parse_dates=['date'])\n",
    "    elif extension == '.json':\n",
    "        return pd.read_json(path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {extension}. Only .csv and .json are supported.\")\n",
    "\n",
    "# Test with CSV\n",
    "print(\"Loading from CSV:\")\n",
    "df_csv_test = load_orders('../data/orders.csv')\n",
    "print(df_csv_test.head())\n",
    "print()\n",
    "\n",
    "# Test with JSON\n",
    "print(\"Loading from JSON (records):\")\n",
    "df_json_test = load_orders('../data/orders_records.json')\n",
    "print(df_json_test.head())\n",
    "print()\n",
    "\n",
    "print(\"Function successfully loads both CSV and JSON formats!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78c1c59-6b6a-4933-a262-9fe9296d3e99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
